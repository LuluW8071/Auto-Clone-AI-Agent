# How It Works

1. **Load Data**  
   Begin by loading the data you want to process.

2. **Pass Data to First LLM**  
   Send the loaded data to the first Large Language Model (LLM).

3. **Process Output with Second LLM**  
   Take the result from the first LLM and pass it to a second LLM. Save the final result to a file.

## Future TODOs

- [ ] **Integrate Claude API**  
  Add support for the Claude API.

- [ ] **Integrate Code Generation Response**  
  Integrate the code generation responses from the Claude API with the first LLM reader, if needed, for Retrieval-Augmented Generation (RAG).

- [x] **Parse Generated Code**  
  Parse the code generated by the first LLM using tools provided by `llama_index`, check for errors, and perform up to three retries if necessary.

- [x] **Save Code**  
  Save the parsed code to the output directory.

### Applications

- [x] **Code Completion Model**  
  The system can also be used as a custom code completion model (Work In Progress).

## Frameworks Used

- [x] **Llama Index**  
- [x] **Ollama**

### Usage

1. **Llama Cloud API Key**  
   Obtain your API key from [Llama Cloud API](https://cloud.llamaindex.ai/).

   ```python
   LLAMA_CLOUD_API_KEY = "YOUR_API_KEY"
   ```

2. **Install requirements**

   ```bash
   pip3 install -r requirements.txt
   ```

3. **Ollama Models**
    Visit the [Ollama GitHub page](https://github.com/ollama/ollama) to explore and try out newer models. \
    > Install ollama and run the model in terminal to download it.
    > Note that models with more parameters may consume more resources and have higher latency. For optimal use case, I chose 7B parameters model.

    ```python
    from ollama import Ollama

    llm1 = Ollama(model="mistral")
    llm2 = Ollama(model="llama3")
    ```